## Lexical Semantics:

### **Lemmas and Senses**
- **Lemmas**: The base or dictionary form of a word (e.g., "run" is the lemma of "ran").
- **Senses**: Different meanings or interpretations of a word. Each lemma can have multiple senses depending on the context.

### **The Linguistic Principle of Contrast**
- This principle states that differences in forms (e.g., sounds, spelling, structure) correlate with differences in meaning.
  
### **Difference in Forms â†’ Difference in Meaning**
- A change in the form of a word (e.g., singular vs. plural, tense) usually indicates a change in its meaning.

---

### **Relation: Word Relatedness**
- The relationship between words based on their meanings, such as synonyms, antonyms, or hypernyms.

### **Semantic Field**
- A group of words related by their meanings, often revolving around a specific concept or category (e.g., the semantic field of "emotion" includes words like "happiness," "anger," "sadness").

![image](https://github.com/user-attachments/assets/bc4e441b-7c43-40cc-a089-c98041dd28d2)


### **Antonymy**
- The relationship between words that have opposite meanings. Examples: "hot" vs. "cold", "up" vs. "down".

![image](https://github.com/user-attachments/assets/adcd55cb-4c1a-4bca-8a2d-be1049b1925f)


### **Connotation**
- The additional meaning or nuance that a word carries beyond its literal definition. For example, the word "home" has connotations of warmth, family, and security.

![image](https://github.com/user-attachments/assets/d85f16e3-7e30-45e4-881d-d18440ee1f84)


---

### **Fully Understanding: Vector Learning in Word2Vec**
- **Word2Vec** uses vectors to represent words in a high-dimensional space. The algorithm learns word representations by training on large text corpora and attempts to predict a word based on its context (or vice versa).
  
  - **Skip-gram Model**: The model predicts the context given a target word.
  - **Continuous Bag of Words (CBOW)**: The model predicts the target word from the surrounding context.

![image](https://github.com/user-attachments/assets/86184775-a6d6-4197-bd55-44c6912e04cd)
![image](https://github.com/user-attachments/assets/24c96038-9405-4e1d-bc07-d304d9b96475)

---

### **Caveats with Parallelogram Method**
- The parallelogram method, often used in vector-based approaches, can have limitations like:
  - **Context Dependence**: Word meanings may vary significantly depending on the context in which they are used.
  - **Quality of Training Data**: Inaccuracies or biases in the corpus can affect the quality of word embeddings.

![image](https://github.com/user-attachments/assets/0074339d-0422-4365-88c7-60cd979f6b65)

![image](https://github.com/user-attachments/assets/41b857dc-0a83-474e-b9ed-fec33ec732d0)


### **Biases in Embeddings**
- Word embeddings can inherit biases present in the training data. For example, gender biases in professions (e.g., "doctor" is closer to "male" than to "female") can propagate through embeddings like Word2Vec.

---

## Neural Network Training

### **Computation Graph in Backpropagation**
- In neural network training, a **computation graph** is a directed graph where:
  - Each node represents a mathematical operation.
  - Edges represent data flow between operations.
- **Backpropagation**: A method for updating weights in a neural network by computing the gradient of the loss function with respect to each weight, using the chain rule.
  

https://github.com/user-attachments/assets/fe8ecdfc-7538-4301-a45c-41665d663ceb


The key steps in backpropagation:
1. **Forward Pass**: Calculate output based on input data.
2. **Loss Calculation**: Compare output to target and calculate the loss.
3. **Backward Pass**: Compute gradients by propagating the error backward through the network.
4. **Weight Update**: Update the weights using gradient descent or a similar optimization algorithm.

