# Ranked Retrieval Model

## **Feast and Famine Problem**
The **feast and famine problem** refers to the issue of having either too many or too few relevant documents for a query. It can occur when a retrieval model overly favors certain terms, leading to either an overload of results (feast) or a lack of results (famine).

### **Free Text Queries**
- Queries in natural language, allowing flexibility in input.
- May lead to more diverse results but also less precision due to ambiguity.

### **Jaccard Coefficient** (Issues)
The Jaccard coefficient is used to measure similarity between two sets.  
Formula:  
$\text{Jaccard}(A, B) = \frac{|A \cap B|}{|A \cup B|}$  

**Issues**:  
- Sensitive to set size.
- Can overestimate similarity when the sets are large.

### **Term-Document Count Matrix** - Bag of Words
- A matrix where rows represent documents, and columns represent terms (words).
- Values represent term frequency (how many times a term appears in a document).

### **Term-Frequency Weighting**
- Weighs terms based on their frequency in a document.
- Common term frequency weighting:  
  $TF(t, D) = \frac{\text{Number of occurrences of term t in document D}}{\text{Total terms in D}}$

### **IDF - Inverse Document Frequency Weighting**
- Measures how rare a term is across a collection of documents.  
  Formula:  
  $IDF(t) = \log \left( \frac{N}{df(t)} \right)$  
  where $N$ is the total number of documents and $df(t)$ is the number of documents containing the term $t$.

### **Collection Frequency vs. Social Frequency**
- **Collection Frequency**: The number of times a term appears across the entire document collection.
- **Social Frequency**: Refers to how frequently a term is used within a social context or specific subgroup.

---

# Vector Space Models

## **Query as a Vector**
- In vector space models, both queries and documents are represented as vectors in a multi-dimensional space where each dimension corresponds to a term in the collection.

## **Why Not Euclidean Distance?**
- **Euclidean distance** is not ideal for text data because it does not handle differences in length and magnitude well.
- **Cosine Similarity** is used instead, as it measures the cosine of the angle between two vectors, making it more suited for text comparison.

### **Cosine Similarity**:  
Formula:  
$\text{cosine similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}$  
Where $A$ and $B$ are vectors representing two documents or a query and a document.

---

# Length Normalization

### **Length Normalization**:  
- **L2 Norm** (Euclidean norm) is used to normalize vectors, ensuring that all vectors have a magnitude of 1. This allows for comparisons based on direction rather than magnitude.
- Normalizing vectors makes the vectors have a **unit length**, which simplifies similarity calculations.  

Formula for **L2 Normalization**:  

<img width="459" alt="image" src="https://github.com/user-attachments/assets/86e59295-fe94-48d6-8f81-8f5754ddd540" />



Where $A_i$ is the $i$-th component of the vector.

### **Length Normalizing** = Making Unit Vectors  
Unit vectors ensure that the angle between them can be used for similarity comparisons, removing the influence of the vector's magnitude.

---

## **Calculating TF-IDF Cosine Scores**
- This involves computing the term frequency-inverse document frequency (TF-IDF) for each term and then calculating the cosine similarity between the query and documents using the TF-IDF values.

---

# Evaluating Search Engines

## **Evaluating Search Engines**
1. **Precision** and **Recall** (as discussed earlier) are crucial for evaluating the performance of search engines.
2. **F-measure** combines both precision and recall into one metric:  
  $F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

---

# Named Entity Recognition (NER)

## **Morphenes**
- A **morpheme** is the smallest meaningful unit in a language that cannot be further divided.  
  Example: "incoming" consists of two morphemes: "in-" and "coming".

## **Why is NER Hard?**
- **Segmentation**: Correctly splitting text into entities.
- **Type Ambiguity**: Words can belong to multiple categories, making it challenging to classify them correctly.

### **BIO-Tagging** / **IO-Tagging** / **BIOES-Tagging**:
- **BIO-tagging**: Classifies tokens as **Beginning**, **Inside**, or **Outside** an entity.  
  Example:  
  - "Steve" (B-PER)  
  - "Jobs" (I-PER)  
  - "is" (O)  

- **IO-tagging**: Similar to BIO, but lacks the "Beginning" tag, using only **Inside** and **Outside** labels.

- **BIOES-tagging**: Includes **End** and **Single** tags for more precise entity boundaries.  
  Example:  
  - "Steve" (B-PER)  
  - "Jobs" (E-PER)  
  - "is" (O)

---

## **Open vs. Closed Words in POS Tagging**

- **Open Words**: Words that can take on new meanings or forms in different contexts (e.g., nouns, verbs, adjectives).
- **Closed Words**: Words that are limited to a specific function (e.g., prepositions, conjunctions, articles).
