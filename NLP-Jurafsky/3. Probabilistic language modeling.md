# 3. Probabilistic Language Modeling

Language models (LMs) predict the probability of a sequence of words, enabling applications like text generation, speech recognition, and machine translation.

---

## 3.1 Introduction to N-grams

### N-grams and Markov's Assumption
An **N-gram** is a contiguous sequence of $N$ words. The **Markov assumption** simplifies prediction by assuming that the probability of a word depends only on the preceding $(N-1)$ words.

#### Example:
For the sentence "I love natural language processing":
- Unigrams ($N=1$): "I", "love", "natural", "language", "processing".
- Bigrams ($N=2$): "I love", "love natural", "natural language", "language processing".
- Trigrams ($N=3$): "I love natural", "love natural language", "natural language processing".

For a bigram model:
$P(\text{"I love natural"}) \approx P(\text{"love"} | \text{"I"}) \cdot P(\text{"natural"} | \text{"love"})$

If $P(\text{"love"} | \text{"I"}) = 0.3$ and $P(\text{"natural"} | \text{"love"}) = 0.2$, then:
$P(\text{"I love natural"}) = 0.3 \cdot 0.2 = 0.06$

---

## 3.2 Estimating N-gram Probabilities

The probability of an N-gram is estimated using **maximum likelihood estimation (MLE)**:

$P(w_i | w_{i-(N-1)}^{i-1}) = \frac{\text{Count}(w_{i-(N-1)}^i)}{\text{Count}(w_{i-(N-1)}^{i-1})}$

#### Example:
In a corpus, the bigram "love natural" appears 5 times, and the unigram "love" appears 20 times. Using MLE:
$P(\text{"natural"} | \text{"love"}) = \frac{\text{Count}(\text{"love natural"})}{\text{Count}(\text{"love"})} = \frac{5}{20} = 0.25$

---

## Perplexity

### Definition
**Perplexity** measures how well a model predicts a text. It is the inverse probability of the test set normalized by the number of words:
$\text{Perplexity} = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i)}$

#### Example:
For a sentence with probabilities $P(w_1) = 0.1$, $P(w_2) = 0.2$, and $P(w_3) = 0.05$:
$\text{Perplexity} = 2^{-\frac{1}{3} \left( \log_2(0.1) + \log_2(0.2) + \log_2(0.05) \right)}$

If perplexity is low (e.g., 30), the model predicts well. A high perplexity (e.g., 100) indicates poor performance.

---

## Shannon Visualization Method

The **Shannon Visualization Method** generates text by iteratively sampling words based on their conditional probabilities.

#### Example:
Given a bigram model:
- $P(\text{"I"} | \text{"START"}) = 0.4$
- $P(\text{"love"} | \text{"I"}) = 0.6$
- $P(\text{"natural"} | \text{"love"}) = 0.5$

The model might generate: "I love natural language."

---

## Zero Probability Problems in N-grams

### Problem
When an N-gram is not in the training data, its probability is zero, making the entire sentence probability zero.

### Solution: Smoothing Techniques

#### Laplace Smoothing
Adds one to all counts:
$P(w_i | w_{i-(N-1)}^{i-1}) = \frac{\text{Count}(w_{i-(N-1)}^i) + 1}{\text{Count}(w_{i-(N-1)}^{i-1}) + V}$

#### Example:
If "language processing" appears 0 times, and the vocabulary size $V = 1000$:
$P(\text{"processing"} | \text{"language"}) = \frac{0 + 1}{\text{Count}(\text{"language"}) + 1000}$

---

## Backoff and Interpolation

### Backoff Models
Use lower-order N-grams when higher-order N-grams have zero probability.

#### Example: Stupid Backoff
If $P(\text{"processing"} | \text{"natural language"})$ is zero, backoff to:
$P(\text{"processing"} | \text{"language"})$

### Interpolation Models
Combine probabilities from different N-gram levels:
$P(w_i | w_{i-(N-1)}^{i-1}) = \lambda_N P(w_i | w_{i-(N-1)}^{i-1}) + \lambda_{N-1} P(w_i | w_{i-(N-2)}^{i-1}) + \cdots$

#### Example:
Assign $\lambda_3 = 0.6$, $\lambda_2 = 0.3$, $\lambda_1 = 0.1$. If:
- $P(w_i | w_{i-2}^{i-1}) = 0.4$
- $P(w_i | w_{i-1}) = 0.2$
- $P(w_i) = 0.1$

Then:
$P(w_i) = 0.6 \cdot 0.4 + 0.3 \cdot 0.2 + 0.1 \cdot 0.1 = 0.27$

---

### **Markov's Assumption**

Markov's Assumption is a simplifying principle used in probability and stochastic processes, particularly in the context of **Markov Chains**. It asserts that the future state of a system depends only on its current state, not on the sequence of states that preceded it.

### **Mathematical Formulation**

For a stochastic process \(\{X_t\}\), Markov's Assumption can be written as:

\[
P(X_{t+1} | X_t, X_{t-1}, \dots, X_0) = P(X_{t+1} | X_t)
\]

This means the probability of transitioning to the next state \(X_{t+1}\) is determined solely by the current state \(X_t\), and not by the history of earlier states.

---


## Kneser-Ney Smoothing

Kneser-Ney smoothing is a widely used smoothing technique in natural language processing, particularly in language modeling. It enhances probability estimates by considering not only the frequency of words but also the diversity of contexts in which they occur. This makes it especially effective for handling rare or unseen n-grams.

Kneser-Ney smoothing is particularly powerful because it shifts the probability mass from frequent events to less frequent ones based on their contextual richness. This makes it a preferred choice in modern n-gram language models for generating more realistic and accurate predictions.

The formula for Kneser-Ney smoothing is:

$P_{\text{KN}}(w_i | w_{i-(N-1)}^{i-1}) = \frac{\max(\text{Count}(w_{i-(N-1)}^i) - \delta, 0)}{\text{Count}(w_{i-(N-1)}^{i-1})} + \alpha \cdot P_{\text{lower}}(w_i)$


**Key Components:**
1. **Discounting $(\(\delta\))$**: Subtracts a small value from the raw count to prevent overestimating probabilities for frequent n-grams.
2. **Continuation Probability $(\(P_{\text{lower}}(w_i)\))$**: Measures how often a word appears as part of different contexts, reflecting the diversity of usage.
3. **Normalization $(\(\alpha\))$**: Ensures probabilities sum to 1 and balances the influence of higher-order and lower-order models.

---

## Summary Table

| Technique                          | Key Idea                                | Example                              |
|------------------------------------|-----------------------------------------|--------------------------------------|
| **Laplace Smoothing**              | Adds one to all counts.                | Ensures $P > 0$ for unseen N-grams. |
| **Absolute Discounting**           | Subtracts a fixed discount.            | Balances high and low-frequency words. |
| **Stupid Backoff**                 | Uses lower-order models when needed.   |  $P(\text{"processing"} \| \text{"language"})$ as fallback. |
| **Kneser-Ney Smoothing**           | Considers context diversity.           | Assigns better probabilities to rare contexts. |

With these examples and explanations, the mathematical formulas are more intuitive and demonstrate practical applications.
