## Regex and Patterns

### **1. Regex Rules:**
**Regex** (Regular Expressions) is used for pattern matching in text.  
Here are some basic and common patterns:  

| **Pattern**       | **Explanation**                               | **Example**                           |
|--------------------|-----------------------------------------------|---------------------------------------|
| `\d+`             | Matches one or more digits                   | "123" matches; "abc" does not         |
| `[a-z]+`          | Matches one or more lowercase letters        | "abc" matches; "ABC" does not         |
| `[A-Za-z]+`       | Matches one or more letters (any case)       | "Hello" matches; "123" does not       |
| `\bword\b`        | Matches whole word "word"                    | "word" matches; "sword" does not      |
| `\w+`             | Matches one or more word characters          | "word1" matches; "%&" does not        |
| `[^a-zA-Z]`       | Matches non-alphabetical characters          | "123" matches; "abc" does not         |
| `(abc|def)`       | Matches "abc" or "def"                       | "abc" matches; "ghi" does not         |

---

### **2. Regex Substitution:**
- Substitution replaces matched patterns with specified replacements.  
  Example (Python):  
  ```python
  import re
  text = "I love cats and dogs."
  result = re.sub(r'cats|dogs', 'animals', text)
  print(result)  # Output: "I love animals and animals."
  ```

---

### **3. Lemmatization (Lemma):**
**Definition:**  
Lemmatization reduces a word to its base form (lemma) using vocabulary and grammar rules.  

**Example:**  
| **Word Form** | **Lemma** |
|---------------|-----------|
| "running"     | "run"     |
| "better"      | "good"    |
| "studies"     | "study"   |

- **Small Note:** Lemmatization considers context (e.g., part of speech).

---

### **4. Stemming (Stem):**
**Definition:**  
Stemming reduces a word to its root form by chopping off affixes. It is rule-based and may not produce valid words.  

**Example:**  
| **Word Form** | **Stem** |
|---------------|----------|
| "running"     | "run"    |
| "studies"     | "studi"  |
| "happiness"   | "happi"  |

---

### **5. Porter Stemmer:**
- The **Porter Stemming Algorithm** is one of the most common stemming techniques.
- Developed by Martin Porter in 1980.
- Operates in five phases, applying rules like:
  - Removing plurals: `caresses` → `caress`
  - Removing suffixes: `running` → `run`

**Python Example:**  
```python
from nltk.stem import PorterStemmer
ps = PorterStemmer()
print(ps.stem("running"))  # Output: "run"
```

---

### **6. Word Forms (Small Notes):**
- Word forms include variations of a base word:
  - **Inflections:** Plural/singular, tense (e.g., "dog", "dogs"; "run", "ran").
  - **Derivatives:** Formed by adding prefixes or suffixes (e.g., "happiness" → "happy").
- Understanding word forms is critical for NLP tasks like text normalization.

--- 
<br>
<br>

## Tokenization and Vocabulary growth size

### **1. Heap’s Law and Herdan’s Law: Vocabulary Size Growth**
---

#### **Heap’s Law**  
Heap’s Law describes the relationship between the size of a text corpus and the growth of its vocabulary.  

The formula for Heap's Law is $V(n) = K \cdot n^\beta$, where:
- $V(n)$ is the vocabulary size.
- $n$ is the number of tokens.
- $K$ and $\beta$ are constants.

**Key Insight:**  
- Vocabulary grows sub-linearly with the size of the corpus, meaning the rate of encountering new words decreases as text size increases.
- Typically K is between 10 and 100, and β is between 0.4 and 0.6.

---

#### **Herdan’s Law**  
Herdan’s Law is similar to Heap’s Law and states that the growth of vocabulary follows a power-law distribution. It is often used interchangeably with Heap’s Law, but the emphasis is on the mathematical foundation of vocabulary distribution in natural language.  

---

### **2. Clitics**
Clitics are morphemes (smallest meaningful units) that cannot stand alone as independent words and attach to a host word.  

**Examples:**  
- **English:**  
  - "I'm" → "I am" (clitic: "'m")  
  - "They'll" → "They will" (clitic: "'ll")  
- **French:**  
  - "l'arbre" → "le arbre" (clitic: "l'")  

**Note:**  
Clitics differ from affixes (prefixes/suffixes) because they retain some syntactic independence.

---

# Tokenization Pipeline in Transformers

This note provides a high-level overview of the tokenization pipeline used in Transformers. The pipeline includes **Normalization** and **Pre-tokenization**, which are essential preprocessing steps before applying subword tokenization algorithms like Byte-Pair Encoding (BPE), WordPiece, or Unigram.

## 3. Normalization

The normalization step performs general text cleanup, including:  
- Removing extra whitespace.  
- Lowercasing (optional, based on the tokenizer).  
- Removing accents.  
- Applying Unicode normalization (e.g., NFC, NFKC).

## Unicode Normalization Forms

Unicode normalization is used to standardize text representations by converting different but equivalent sequences of characters into a consistent format. The four main forms are:

### 1. NFC (Normalization Form C)
- Combines **canonical equivalent** characters into their **composed form**.
- Example: Converts "e" + "́" (combining acute accent) to "é" (precomposed character).

#### Use Case:
Preferred when storage efficiency or compatibility with precomposed characters is required.

---

### 2. NFD (Normalization Form D)
- Splits characters into their **decomposed form**.
- Example: Converts "é" (precomposed) to "e" + "́" (base character + combining accent).

#### Use Case:
Useful for systems that need access to individual components of a character.

---

### 3. NFKC (Normalization Form KC)
- Similar to NFC but also applies **compatibility decomposition** to replace characters with their compatibility equivalents.
- Example: Converts "①" (circled number one) to "1".

#### Use Case:
Used in scenarios where compatibility and visual equivalence are more critical than preserving original encoding.

---

### 4. NFKD (Normalization Form KD)
- Similar to NFD but applies **compatibility decomposition**.
- Example: Converts "①" (circled number one) to "1", and "é" to "e" + "́".

#### Use Case:
Used for text analysis or search where character components or compatibility are important.

---

### Key Points
- **Canonical Equivalence**: Ensures characters that look or function the same are treated equally.
- **Compatibility Equivalence**: Replaces characters with their more standard equivalents for broader compatibility.
- Tools like Python's `unicodedata` library provide functions (`normalize()`) to apply these forms programmatically.


#### Example  
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
# Output: 'hello how are u?'
```

The **bert-base-uncased** tokenizer applies lowercasing and removes accents. For comparison, try the **bert-base-cased** tokenizer to observe the differences.  

## 4. Pre-tokenization

The pre-tokenization step splits raw text into smaller entities (e.g., words). This enables the tokenizer to create subtokens efficiently.  

### Examples  

#### BERT Tokenizer  
Splits text on **whitespace** and **punctuation**.  
```python
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
# Output: [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

#### GPT-2 Tokenizer  
Splits on **whitespace** and **punctuation** but retains spaces with the `Ġ` symbol for decoding.  
```python
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
# Output: [('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)), ('?', (19, 20))]
```

#### T5 Tokenizer  
Splits on **whitespace** and adds a specific token (`▁`) for spaces.  
```python
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
# Output: [('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]
```

### Key Observations  
- **BERT Tokenizer**: Ignores double spaces and focuses on whitespace and punctuation.  
- **GPT-2 Tokenizer**: Retains spaces and represents them with `Ġ`.  
- **T5 Tokenizer**: Keeps spaces with `_` and splits only on whitespace.  

## Next Steps

With a clear understanding of normalization and pre-tokenization, explore the three main subword tokenization algorithms in the following sections:  
- [SentencePiece](#sentencepiece-overview)  
- [Byte-Pair Encoding (BPE)](#byte-pair-encoding-bpe)  
- [WordPiece](#wordpiece-algorithm)  
- [Unigram](#unigram-tokenization)

-------------------------------------------

### **5. Subword Tokenization**
Subword tokenization breaks words into smaller units for language modeling and NLP tasks. This approach is particularly effective for handling:  
- Rare words  
- Morphologically rich languages  


#### **Techniques:**

##### **a. Byte Pair Encoding (BPE)**  
- BPE iteratively merges frequent pairs of symbols (subwords or characters).  
- Common in models like GPT and OpenAI's LLMs.  

**Steps:**  
1. Start with characters as individual units.  
2. Merge the most frequent pair of symbols.  
3. Repeat until a vocabulary limit is reached.  

**Example:**  
For "lower, lowest":  
- Initial: `l o w e r, l o w e s t`  
- Merge `l o w` → `low`  
- Merge `low e r` → `lower`  
- Merge `low e s t` → `lowest`





https://github.com/user-attachments/assets/fba56916-42a8-499a-8814-90848366c82b


[Read the BPE-Blog](https://huggingface.co/learn/nlp-course/en/chapter6/5)


---

##### **b. WordPiece**
- Similar to BPE but uses a probabilistic approach to merge subwords.  
- Common in models like BERT.  

**Key Difference from BPE:**  
- WordPiece optimizes for likelihood in a language model rather than raw frequency.  



https://github.com/user-attachments/assets/6c3a6dc0-1afd-416e-b4a0-e12a961d65de

[Read the WordPiece-Blog](https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt)




---

##### **c. Unigram**
- A probabilistic subword segmentation method.  
- Begins with a large vocabulary and removes the least likely subwords iteratively.  

**Example (Probabilistic Assignment):**  
- "playing" → {"play", "ing"}, {"pl", "ay", "ing"}  
- Chooses segmentation with the highest probability.  



https://github.com/user-attachments/assets/727d0c8f-d6aa-497f-8c3f-58870299ac1b




[Read the Unicode-Blog](https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt)

---

### **6. Morpheme**
A **morpheme** is the smallest grammatical unit in a language that carries meaning.  

**Types of Morphemes:**  
1. **Free Morphemes:** Can stand alone (e.g., "book", "run").  
2. **Bound Morphemes:** Cannot stand alone (e.g., prefixes like "un-", suffixes like "-ing").  

**Significance:**  
- Morphological analysis is central to tokenization, especially in morphologically rich languages.

---



